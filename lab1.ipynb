{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4c3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74cc4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "prompt = \"Explain about Gen_AI in simple terms.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed8a558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain about Gen_AI in simple terms.................................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline('text-generation', model='bert-base-uncased')\n",
    "outputs = text_gen(prompt, max_length=100, num_return_sequences=1)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9cc5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b61ded81-2490-4b60-95c1-07c6c6b25308)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain about Gen_AI in simple terms.\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline('text-generation', model='roberta-base')\n",
    "outputs = text_gen(prompt, max_length=100, num_return_sequences=1)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db835678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain about Gen_AI in simple terms. squatencencenc ingestion Mavericks Mavericks Mavericksurrencies watering galacticgot ShakPatrickPatrick likelihoodencenc NDPencencbay NDP Princeton Princeton squat squatenc Hookencenc Princetonenc watering ingestionencSelencSel workload Princetonenc Alvin workload workload workloadPatrick workload Alvin2014 log workload workload AtomPatrick workloadSelPatrickino sureimbleurgy PrincetoninoSelSelino workload workload]' workload workload Fernand Alvin bandwagonPatrickSel 93]'Sel workload workloadinoSel workload Hopefullyenc workloadetersenceters workloadSel workload Western workloadSeleterseterseters bondage bandwagonSelSeleters workload workload Princeton neuronSelAlertino PrincetonetersSel Princetonetersino Princetoninoeters workloadimble workloadeters PrincetonSeletersSel FundinginoSelimbleSel Hooketerseters FA HooketersSeletersAlertinoinoinoStatus Burns Funding Fundingeters au FueletersetersSel workloadNeithereters stainedeters graveseters FAeters principetersSelekieters Fireterseters bandwagonetersetersinoeters FirreflectetersStatus Pathetersetersekieterseters NDP NDP futuresreflectetersetersreflect &eters workload NDPSeleters785 FueletersSelino Fueleters785eterseters stained moistureetersetersfacedStatuseters debugger Wiley aueterseters illegeterseters Fundingeters Burnseterseters besideeters Burns BurnsAlertAlerteters aueters785785 debugger debuggeretersAlert debugger moistureeters dfeterseters Wiley Palaceeters\n"
     ]
    }
   ],
   "source": [
    "text_gen = pipeline('text-generation', model='facebook/bart-base')\n",
    "outputs = text_gen(prompt, max_length=100, num_return_sequences=1)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65310ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = \"The goal of Generative AI is to [MASK] new content.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fddc51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Fill-Mask Results:\n",
      "Token: create, Score: 0.539692759513855\n",
      "Token: generate, Score: 0.15575766563415527\n",
      "Token: produce, Score: 0.05405496060848236\n",
      "Token: develop, Score: 0.044515229761600494\n",
      "Token: add, Score: 0.017577484250068665\n"
     ]
    }
   ],
   "source": [
    "mask_fill = pipeline('fill-mask', model='bert-base-uncased')\n",
    "results = mask_fill(promt)\n",
    "print(\"BERT Fill-Mask Results:\")\n",
    "for result in results:\n",
    "    print(f\"Token: {result['token_str']}, Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3b07fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Fill-Mask Results:\n",
      "Token:  generate, Score: 0.3711\n",
      "Token:  create, Score: 0.3677\n",
      "Token:  discover, Score: 0.0835\n",
      "Token:  find, Score: 0.0213\n",
      "Token:  provide, Score: 0.0165\n"
     ]
    }
   ],
   "source": [
    "mask_fill = pipeline('fill-mask', model='roberta-base')\n",
    "results = mask_fill(promt)\n",
    "print(\"RoBERTa Fill-Mask Results:\")\n",
    "for result in results:\n",
    "    print(f\"Token: {result['token_str']}, Score: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b60a507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa Fill-Mask Results:\n",
      "Token:  create, Score: 0.0746\n",
      "Token:  help, Score: 0.0657\n",
      "Token:  provide, Score: 0.0609\n",
      "Token:  enable, Score: 0.0359\n",
      "Token:  improve, Score: 0.0332\n"
     ]
    }
   ],
   "source": [
    "mask_fill = pipeline('fill-mask', model='facebook/bart-base')\n",
    "results = mask_fill(promt)\n",
    "print(\"RoBERTa Fill-Mask Results:\")\n",
    "for result in results:\n",
    "    print(f\"Token: {result['token_str']}, Score: {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d63a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Machine learning models require large datasets for training. Deep learning uses neural networks with multiple layers.\"\n",
    "question = \"What do models need for training?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09958b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: .\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline('question-answering', model='bert-base-uncased')\n",
    "result = qa(question=question, context=context)\n",
    "print(f\"BERT: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf755df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa: for training. Deep learning uses\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline('question-answering', model='roberta-base')\n",
    "result = qa(question=question, context=context)\n",
    "print(f\"RoBERTa: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3168aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Machine learning models require large datasets for training.\n"
     ]
    }
   ],
   "source": [
    "qa = pipeline('question-answering', model='facebook/bart-base')\n",
    "result = qa(question=question, context=context)\n",
    "print(f\"BART: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86993fee",
   "metadata": {},
   "source": [
    "## Observation Table:\n",
    "\n",
    "Copy this markdown table into your notebook and fill it out based on your experiments.\n",
    "\n",
    "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Generation** | BERT | *Failure* | *Generated random symbols.* | *Encoder Only Model* |\n",
    "| | RoBERTa | *Failure* | *Returns the same promt as output* | *Encoder Only Model* |\n",
    "| | BART | *Sucess* | *Incoherrent text generation* | *Encoder Decoder Model* |\n",
    "| **Fill-Mask** | BERT | *Success* | *Correct Prediction* | *Trained with MLM* |\n",
    "| | RoBERTa | *Success* | *High-quality predictions* | *Optimized MLM* |\n",
    "| | BART | *Partial* | *Inconsistent results* | *Not designed for MLM* |\n",
    "| **QA** | BERT | *Failure* | *Symbol Output* | *Not QA fine-tuned* |\n",
    "| | RoBERTa | *Partial* | *Unstable answer* | *Not QA fine tuning* |\n",
    "| | BART | *Partial* | *Sometimes coherent* | *Not QA fine-tuning* |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
